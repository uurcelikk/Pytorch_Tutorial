{"cells":[{"cell_type":"markdown","metadata":{"id":"1TB4xkSD6C_D"},"source":["###  Getting a Dataset \n","\n","We're going to start with FashionMNIST.The original MNIST dataset contains thousands of examples of handwritten digits (from 0 to 9) and was used to build computer vision models to identify numbers for postal services."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JjPjoltQmFOY","outputId":"dd4a3cd9-1703-4267-8bf9-96b87719289f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 26421880/26421880 [00:01\u003c00:00, 18621598.49it/s]\n"]}],"source":["import torch\n","from torch import nn\n","\n","# import torchvision\n","import torchvision\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","\n","# Setup training data \n","train_data = datasets.FashionMNIST(\n","    root = \"data\",\n","    train = True,\n","    download = True,\n","    transform = ToTensor(),\n","    target_transform=None\n",")\n","\n","# Setup testinf data\n","test_data = datasets.FashionMNIST(\n","    root = \"data\",\n","    train = False,\n","    download = True,\n","    transform = ToTensor()\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BgFTwyVT6HVO"},"outputs":[],"source":["# See first training sample\n","image, label = train_data[0]\n","image,label"]},{"cell_type":"markdown","metadata":{"id":"PB5bddkr7tir"},"source":["### Input and output shapes of a computer vision model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCAX1-IN77Ce"},"outputs":[],"source":["image.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X4P1NGbg8Hng"},"outputs":[],"source":["### Having color_channels=1 means the image is grayscale.\n","#[color_channels=1, height=28, width=28]\n","#If color_channels=3, the image comes in pixel values for red, green and blue(RGB)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OpWgf9Pn-Ph-"},"outputs":[],"source":["\n","len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GULQD-yc-PxT"},"outputs":[],"source":["class_names = train_data.classes\n","class_names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LZT8Fw8_-Wfq"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","image, label = train_data[0]\n","print(f\"image shape: {image.shape}\")\n","plt.imshow(image.squeeze())\n","plt.title(label);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oowcUm9Q-uR9"},"outputs":[],"source":["plt.imshow(image.squeeze(), cmap = \"gray\")\n","plt.title(class_names[label])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSt2PIuu-79T"},"outputs":[],"source":["torch.manual_seed(42)\n","fig = plt.figure(figsize= (9,9))\n","rows, cols = 4, 4\n","for i in range(1,rows * cols +1):\n","  random_idx= torch.randint(0,len(train_data), size = [1]).item()\n","  img, label = train_data[random_idx]\n","  fig.add_subplot(rows,cols,i)\n","  plt.imshow(img.squeeze(), cmap = \"gray\")\n","  plt.title(class_names[label])\n","  plt.axis(False);"]},{"cell_type":"markdown","metadata":{"id":"8ObQ15S6_AUk"},"source":["### Prepare DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0iF9W1Ljvwd2"},"outputs":[],"source":["# batches ya da mini batches adını verdiğimiz parçalara ayırırız. \n","# Çünkü gerçekten büyük verilerle çalışırken forward ya da backward yapmak çok zor olur.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjdTE1V_7rhU"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","BATCH_SIZE = 32\n","\n","# Turn datasets into iterables( batches)\n","train_dataloader = DataLoader(train_data,# dataset to turn into iterable\n","                              batch_size = BATCH_SIZE,#how many samples per batch?\n","                              shuffle = True # shuffle data every epoch?\n","                              )\n","\n","test_dataloader = DataLoader(test_data,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False # don't necessarily have to shuffle the testing data\n",")\n","\n","# Let's check out what we've created\n","print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n","print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n","print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pkxWHIAbooey"},"outputs":[],"source":["# Check out what's inside the training dataloader\n","train_features_batch, train_labels_batch = next(iter(train_dataloader))\n","train_features_batch.shape, train_labels_batch.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBvRTgFD8UMI"},"outputs":[],"source":["# show a sample\n","torch.manual_seed(42)\n","\n","random_idx = torch.randint(0,len(train_features_batch), size = [1]).item()\n","img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n","plt.imshow(img.squeeze(), cmap =\"gray\")\n","plt.title(class_names[label])\n","plt.axis(\"Off\");\n","print(f\"Image size: {img.shape}\")\n","print(f\"Label: {label}, label size: {label.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"Xfibz6_xocEF"},"source":["### Build a baseline model\n","nn.Flatten()bir tensörün boyutlarını tek bir vektöre sıkıştırır."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e_LKJHdB_rJX"},"outputs":[],"source":["# Create a flatten layer\n","flatten_model = nn.Flatten()\n","\n","# Get a single sample\n","x = train_features_batch[0]\n","\n","# flatten sample\n","output = flatten_model(x) \n","\n","print(f\"Shape before flattening: {x.shape} --\u003e [color_channels, height,width]\")\n","print(f\"Shape after flattening: {output.shape} --\u003e [color_channels, height*width]\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"67vLieAu_6lf"},"outputs":[],"source":["# bu dönüşümü pixel datalarını tek bir uzun özellik olması için gerçekleştirdik\n","# nn.Linear katmanları girdilerinin özellik vektörleri biçiminde olmasını sever."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sUhhKEnBAq2"},"outputs":[],"source":["from torch import nn\n","class FashionMNISTModelV0(nn.Module):\n","  def __init__(self,input_shape: int, hidden_units: int, output_shape: int):\n","    super().__init__()\n","    self.layer_stack = nn.Sequential(\n","        nn.Flatten(),\n","        nn.Linear(in_features=input_shape, out_features= hidden_units),\n","        nn.Linear(in_features = hidden_units, out_features= output_shape)\n","    )\n","  def forward(self, x):\n","    return self.layer_stack(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-jFJ63mCSHv"},"outputs":[],"source":["torch.manual_seed(42)\n","\n","model_0 = FashionMNISTModelV0(input_shape = 784,\n","                              hidden_units = 10,\n","                              output_shape= len(class_names))\n","model_0.to(\"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"BpWgb5UEHBqs"},"source":["#### Setup loss, optimizer and evaluation metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4wlRIq9H6BV"},"outputs":[],"source":["\n","import requests\n","from pathlib import Path \n","\n","# Download helper functions from Learn PyTorch repo (if not already downloaded)\n","if Path(\"helper_functions.py\").is_file():\n","  print(\"helper_functions.py already exists, skipping download\")\n","else:\n","  print(\"Downloading helper_functions.py\")\n","  # Note: you need the \"raw\" GitHub URL for this to work\n","  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n","  with open(\"helper_functions.py\", \"wb\") as f:\n","    f.write(request.content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r-Olo_07Nag8"},"outputs":[],"source":["# import accuracy metric\n","from helper_functions import accuracy_fn \n","\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(params = model_0.parameters(), lr = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqSInqELNwbn"},"outputs":[],"source":["from timeit import default_timer as timer \n","def print_train_time(start: float, end: float, device: torch.device = None):\n","    \"\"\"Prints difference between start and end time.\n","\n","    Args:\n","        start (float): Start time of computation (preferred in timeit format). \n","        end (float): End time of computation.\n","        device ([type], optional): Device that compute is running on. Defaults to None.\n","\n","    Returns:\n","        float: time between start and end in seconds (higher is longer).\n","    \"\"\"\n","    total_time = end - start\n","    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n","    return total_time\n","    "]},{"cell_type":"markdown","metadata":{"id":"ATRYGLNGWyrd"},"source":["#### Creating a training loop  and training a model on batches of data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d8xR_C1AXPRW"},"outputs":[],"source":["# Import tqdm for progress bar\n","from tqdm.auto import tqdm\n","\n","# Set the seed and start the timer\n","torch.manual_seed(42)\n","train_time_start_on_cpu = timer()\n","\n","# Set the number of epochs (we'll keep this small for faster training times)\n","epochs = 3\n","\n","# Create training and testing loop\n","for epoch in tqdm(range(epochs)):\n","    print(f\"Epoch: {epoch}\\n-------\")\n","    ### Training\n","    train_loss = 0\n","    # Add a loop to loop through training batches\n","    for batch, (X, y) in enumerate(train_dataloader):\n","        model_0.train() \n","        # 1. Forward pass\n","        y_pred = model_0(X)\n","\n","        # 2. Calculate loss (per batch)\n","        loss = loss_fn(y_pred, y)\n","        train_loss += loss # accumulatively add up the loss per epoch \n","\n","        # 3. Optimizer zero grad\n","        optimizer.zero_grad()\n","\n","        # 4. Loss backward\n","        loss.backward()\n","\n","        # 5. Optimizer step\n","        optimizer.step()\n","\n","        # Print out how many samples have been seen\n","        if batch % 400 == 0:\n","            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n","\n","    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n","    train_loss /= len(train_dataloader)\n","    \n","    ### Testing\n","    # Setup variables for accumulatively adding up loss and accuracy \n","    test_loss, test_acc = 0, 0 \n","    model_0.eval()\n","    with torch.inference_mode():\n","        for X, y in test_dataloader:\n","            # 1. Forward pass\n","            test_pred = model_0(X)\n","           \n","            # 2. Calculate loss (accumatively)\n","            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n","\n","            # 3. Calculate accuracy (preds need to be same as y_true)\n","            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n","        \n","        # Calculations on test metrics need to happen inside torch.inference_mode()\n","        # Divide total test loss by length of test dataloader (per batch)\n","        test_loss /= len(test_dataloader)\n","\n","        # Divide total accuracy by length of test dataloader (per batch)\n","        test_acc /= len(test_dataloader)\n","\n","    ## Print out what's happening\n","    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n","\n","# Calculate training time      \n","train_time_end_on_cpu = timer()\n","total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n","                                           end=train_time_end_on_cpu,\n","                                           device=str(next(model_0.parameters()).device))"]},{"cell_type":"markdown","metadata":{"id":"i1s-2zqSghvd"},"source":["### Make predictions and get Model 0 results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U9jg1IMPNVo-"},"outputs":[],"source":["torch.manual_seed(42)\n","\n","def eval_model(model: torch.nn.Module,\n","               data_loader: torch.utils.data.DataLoader,\n","               loss_fn: torch.nn.Module,\n","              accuracy_fn):\n","  loss, acc = 0, 0\n","  model.eval()\n","  with torch.inference_mode():\n","    for X, y in data_loader:\n","\n","      y_pred = model(X)\n","      # Accumulate the loss and accuracy values per batch\n","      loss += loss_fn(y_pred,y)\n","      acc += accuracy_fn(y_true=y,\n","                         y_pred = y_pred.argmax(dim = 1))\n","       # Scale loss and acc to find the average loss/acc per batch\n","      loss /= len(data_loader)\n","      acc /= len(data_loader)\n","  return {\"model_name\": model.__class__.__name__,\n","          \"model_loss\": loss.item(),\n","          \"model_acc\": acc}\n","\n","model_0_results = eval_model(model = model_0, data_loader = test_dataloader,\n","                             loss_fn = loss_fn,accuracy_fn = accuracy_fn)\n","model_0_results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FKtCZ030OdiC"},"outputs":[],"source":["# Setup device agnostic code\n","import torch\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"markdown","metadata":{"id":"9wldDq1ePaOe"},"source":["###  Model 1: Building a better model with non-linearity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GAhDUYtAPgcb"},"outputs":[],"source":["from torch.nn.modules.linear import Linear\n","from torch.nn.modules.container import Sequential\n","from torch import nn\n","class FashionMNISTModelV1(nn.Module):\n","  def __init__(self,input_shape: int, hidden_units: int, output_shape: int):\n","    super().__init__()\n","    self.layer_stack = nn.Sequential(\n","        nn.Flatten(),\n","        nn.Linear(in_features =input_shape, out_features= hidden_units),\n","        nn.ReLU(),\n","        nn.Linear(in_features = hidden_units, out_features = output_shape),\n","        nn.ReLU()\n","    )\n","  def forward(self, x: torch.Tensor):\n","    return self.layer_stack(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TFeFajbhQQGB"},"outputs":[],"source":["torch.manual_seed(42)\n","model_1 = FashionMNISTModelV1(input_shape = 784,\n","                              hidden_units = 10,\n","                              output_shape = len(class_names)).to(device)\n","\n","next(model_1.parameters()).device # check model device"]},{"cell_type":"markdown","metadata":{"id":"34BGlLPRQvVh"},"source":["####  Setup loss, optimizer and evaluation metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Iui1XCWRJPP"},"outputs":[],"source":["from helper_functions import accuracy_fn\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(params = model_1.parameters(),\n","                            lr = 0.1)"]},{"cell_type":"markdown","metadata":{"id":"XL09tKaJRVQB"},"source":["#### Functionizing training and test loops"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NqHAR0iTRusj"},"outputs":[],"source":["def train_step(model: torch.nn.Module,\n","               data_loader: torch.utils.data.DataLoader,\n","               loss_fn: torch.nn.Module,\n","               optimizer: torch.optim.Optimizer,\n","               accuracy_fn,\n","               device: torch.device = device):\n","    train_loss, train_acc = 0, 0\n","    model.to(device)\n","    for batch, (X, y) in enumerate(data_loader):\n","        # Send data to GPU\n","        X, y = X.to(device), y.to(device)\n","\n","        # 1. Forward pass\n","        y_pred = model(X)\n","\n","        # 2. Calculate loss\n","        loss = loss_fn(y_pred, y)\n","        train_loss += loss\n","        train_acc += accuracy_fn(y_true=y,\n","                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -\u003e pred labels\n","\n","        # 3. Optimizer zero grad\n","        optimizer.zero_grad()\n","\n","        # 4. Loss backward\n","        loss.backward()\n","\n","        # 5. Optimizer step\n","        optimizer.step()\n","\n","    # Calculate loss and accuracy per epoch and print out what's happening\n","    train_loss /= len(data_loader)\n","    train_acc /= len(data_loader)\n","    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n","\n","def test_step(data_loader: torch.utils.data.DataLoader,\n","              model: torch.nn.Module,\n","              loss_fn: torch.nn.Module,\n","              accuracy_fn,\n","              device: torch.device = device):\n","    test_loss, test_acc = 0, 0\n","    model.to(device)\n","    model.eval() # put model in eval mode\n","    # Turn on inference context manager\n","    with torch.inference_mode(): \n","        for X, y in data_loader:\n","            # Send data to GPU\n","            X, y = X.to(device), y.to(device)\n","            \n","            # 1. Forward pass\n","            test_pred = model(X)\n","            \n","            # 2. Calculate loss and accuracy\n","            test_loss += loss_fn(test_pred, y)\n","            test_acc += accuracy_fn(y_true=y,\n","                y_pred=test_pred.argmax(dim=1) # Go from logits -\u003e pred labels\n","            )\n","        \n","        # Adjust metrics and print out\n","        test_loss /= len(data_loader)\n","        test_acc /= len(data_loader)\n","        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HCjmJAy-T_0j"},"outputs":[],"source":["torch.manual_seed(42)\n","\n","from timeit import default_timer as timer\n","train_time_start_on_gpu = timer()\n","\n","epochs = 3\n","for epoch in tqdm(range(epochs)):\n","  print(f\"Epoch: {epoch}\\ n------\" )\n","  train_step(data_loader =train_dataloader,\n","             model = model_1,\n","             loss_fn = loss_fn, optimizer = optimizer,\n","             accuracy_fn = accuracy_fn)\n","  test_step(data_loader = test_dataloader,\n","            model = model_1,\n","            loss_fn = loss_fn,\n","            accuracy_fn = accuracy_fn)\n","\n","train_time_end_on_gpu = timer()\n","total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n","                                            end=train_time_end_on_gpu,\n","                                            device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYm23blDaBue"},"outputs":[],"source":["# torch.manual_seed(42)\n","\n","# # Note: This will error due to `eval_model()` not using device agnostic code \n","# model_1_results = eval_model(model=model_1, \n","#     data_loader=test_dataloader,\n","#     loss_fn=loss_fn, \n","#     accuracy_fn=accuracy_fn) \n","# model_1_results \n","\n","### run time error\n","\n","\n","# Move values to device\n","torch.manual_seed(42)\n","def eval_model(model: torch.nn.Module, \n","               data_loader: torch.utils.data.DataLoader, \n","               loss_fn: torch.nn.Module, \n","               accuracy_fn, \n","               device: torch.device = device):\n","  loss, acc = 0, 0 \n","  model.eval()\n","  with torch.inference_mode():\n","    for X, y in data_loader:\n","      X, y = X.to(device),y.to(device)\n","      y_pred = model(X)\n","      loss += loss_fn(y_pred,y)\n","      acc += accuracy_fn(y_true = y,\n","                         y_pred =y_pred.argmax(dim = 1))\n","      \n","      # Scale loss and acc\n","      loss /= len(data_loader)\n","      acc /= len(data_loader)\n","\n","    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n","            \"model_loss\": loss.item(),\n","            \"model_acc\": acc}\n","\n","# Calculate model 1 results with device-agnostic code \n","model_1_results = eval_model(model=model_1, data_loader=test_dataloader,\n","    loss_fn=loss_fn, accuracy_fn=accuracy_fn,\n","    device=device\n",")\n","model_1_results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMlk5xEyadki"},"outputs":[],"source":["# Check baseline results\n","model_0_results"]},{"cell_type":"markdown","metadata":{"id":"_XouCt5zeXMp"},"source":["### Model 2:Building a Convolutional Neural Network (CNN)¶"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMob5eQAg77M"},"outputs":[],"source":["# Create a convolutional neural network \n","class FashionMNISTModelV2(nn.Module):\n","    \"\"\"\n","    Model architecture copying TinyVGG from: \n","    https://poloclub.github.io/cnn-explainer/\n","    \"\"\"\n","    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n","        super().__init__()\n","        self.block_1 = nn.Sequential(\n","            nn.Conv2d(in_channels=input_shape, \n","                      out_channels=hidden_units, \n","                      kernel_size=3, # how big is the square that's going over the image?\n","                      stride=1, # default\n","                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=hidden_units, \n","                      out_channels=hidden_units,\n","                      kernel_size=3,\n","                      stride=1,\n","                      padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2,\n","                         stride=2) # default stride value is same as kernel_size\n","        )\n","        self.block_2 = nn.Sequential(\n","            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2)\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            # Where did this in_features shape come from? \n","            # It's because each layer of our network compresses and changes the shape of our inputs data.\n","            nn.Linear(in_features=hidden_units*7*7, \n","                      out_features=output_shape)\n","        )\n","    \n","    def forward(self, x: torch.Tensor):\n","        x = self.block_1(x)\n","        # print(x.shape)\n","        x = self.block_2(x)\n","        # print(x.shape\n","        x = self.classifier(x)\n","        # print(x.shape)\n","        return x\n","\n","torch.manual_seed(42)\n","model_2 = FashionMNISTModelV2(input_shape=1, \n","    hidden_units=10, \n","    output_shape=len(class_names)).to(device)\n","model_2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pIJG4cMyidu4"},"outputs":[],"source":["torch.manual_seed(42)\n","\n","# Create sample batch of random numbers with same size as image batch\n","images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\n","test_image = images[0] # get a single image for testing\n","print(f\"Image batch shape: {images.shape} -\u003e [batch_size, color_channels, height, width]\")\n","print(f\"Single image shape: {test_image.shape} -\u003e [color_channels, height, width]\") \n","print(f\"Single image pixel values:\\n{test_image}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OfiwpDET2WzO"},"outputs":[],"source":["torch.manual_seed(42)\n","\n","# Create a convolutional layer with same dimensions as TinyVGG \n","# (try changing any of the parameters and see what happens)\n","conv_layer = nn.Conv2d(in_channels=3,\n","                       out_channels=10,\n","                       kernel_size=3,\n","                       stride=1,\n","                       padding=0) # also try using \"valid\" or \"same\" here \n","\n","# Pass the data through the convolutional layer\n","conv_layer(test_image) # Note: If running PyTorch \u003c1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtHyMdu6kVw9"},"outputs":[],"source":["\n","#If we try to pass a single image in, we get a shape mismatch error:\n","\n","# Add extra dimension to test image\n","test_image.unsqueeze(dim=0).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RkgqQpwdtg5g"},"outputs":[],"source":["# Pass test image with extra dimension through conv_layer\n","conv_layer(test_image.unsqueeze(dim=0)).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bBluEiKgtiQV"},"outputs":[],"source":["torch.manual_seed(42)\n","\n","\n","conv_layer_2 = nn.Conv2d(in_channels =3,\n","                         out_channels = 10,\n","                         kernel_size = (5,5),\n","                         stride = 2,\n","                         padding= 0)\n","\n","conv_layer_2(test_image.unsqueeze(dim = 0)).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJtU1HQD42sa"},"outputs":[],"source":["# Check out the conv_layer_2 internal parameters\n","print(conv_layer_2.state_dict())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tKCr1Svb4oUk"},"outputs":[],"source":["# Get shapes of weight and bias tensors within conv_layer_2\n","print(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -\u003e [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\n","print(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -\u003e [out_channels=10]\")"]},{"cell_type":"markdown","metadata":{"id":"T1jA-NUJ48qZ"},"source":["#### Stepping through nn.MaxPool\n","The kernel_size of the nn.MaxPool2d() layer will effects the size of the output shape.\n","\n","In our case, the shape halves from a 62x62 image to 31x31 image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Y2LVZOu5qMO"},"outputs":[],"source":["print(f\"Test image original shape: {test_image.shape}\")\n","print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n","\n","#create a sample nn.maxPool2d() layer\n","max_pool_layer = nn.MaxPool2d(kernel_size = 2)\n","\n","# Pass data thtough just the conv_layer\n","test_image_through_conv = conv_layer(test_image.unsqueeze(dim = 0))\n","print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n","\n","# pass data throught the max pool layer\n","test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\n","print(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFyb6ROf6cXo"},"outputs":[],"source":["torch.manual_seed(42)\n","\n","random_tensor = torch.randn(size = (1,1,2,2))\n","print(f\"Random tensor:\\n{random_tensor}\")\n","print(f\"Random tensor shape: {random_tensor.shape}\")\n","\n","max_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value \n","\n","# Pass the random tensor through the max pool layer\n","max_pool_tensor = max_pool_layer(random_tensor)\n","print(f\"\\nMax pool tensor:\\n{max_pool_tensor} \u003c- this is the maximum value from random_tensor\")\n","print(f\"Max pool tensor shape: {max_pool_tensor.shape}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0MlbgLpt7K5V"},"outputs":[],"source":["# Setup loss and optimizer\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(params = model_2.parameters(),\n","                            lr = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vYDXQq3NNLSb"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6e59631681746189a52af5c4f8f5050","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0\n","---------\n","Train loss: 0.59518 | Train accuracy: 78.38%\n","Test loss: 0.39500 | Test accuracy: 85.99%\n","\n","Epoch: 1\n","---------\n","Train loss: 0.36536 | Train accuracy: 86.90%\n","Test loss: 0.35244 | Test accuracy: 86.91%\n","\n","Epoch: 2\n","---------\n","Train loss: 0.32588 | Train accuracy: 88.13%\n","Test loss: 0.32719 | Test accuracy: 88.07%\n","\n","Train time on cpu: 211.551 seconds\n"]}],"source":["\n","torch.manual_seed(42)\n","\n","# Measure time\n","from timeit import default_timer as timer\n","train_time_start_model_2 = timer()\n","\n","# Train and test model \n","epochs = 3\n","for epoch in tqdm(range(epochs)):\n","    print(f\"Epoch: {epoch}\\n---------\")\n","    train_step(data_loader=train_dataloader, \n","        model=model_2, \n","        loss_fn=loss_fn,\n","        optimizer=optimizer,\n","        accuracy_fn=accuracy_fn,\n","        device=device\n","    )\n","    test_step(data_loader=test_dataloader,\n","        model=model_2,\n","        loss_fn=loss_fn,\n","        accuracy_fn=accuracy_fn,\n","        device=device\n","    )\n","\n","train_time_end_model_2 = timer()\n","total_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n","                                           end=train_time_end_model_2,\n","                                           device=device)"]},{"cell_type":"markdown","metadata":{"id":"AaFYP5JeNsDl"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zGuzj8IVOIuk"},"outputs":[{"data":{"text/plain":["{'model_name': 'FashionMNISTModelV2',\n"," 'model_loss': 0.0006759435054846108,\n"," 'model_acc': 0.320257331366464}"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["# get model_2 results\n","model_2_results = eval_model(\n","    model = model_2,\n","    data_loader = test_dataloader,\n","    loss_fn = loss_fn,\n","    accuracy_fn = accuracy_fn\n",")\n","model_2_results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hemoHznKWNV3"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-d699262f-d3b1-4ff9-bcae-9271815916ac\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003emodel_name\u003c/th\u003e\n","      \u003cth\u003emodel_loss\u003c/th\u003e\n","      \u003cth\u003emodel_acc\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003eFashionMNISTModelV0\u003c/td\u003e\n","      \u003ctd\u003e0.001061\u003c/td\u003e\n","      \u003ctd\u003e0.300289\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eFashionMNISTModelV1\u003c/td\u003e\n","      \u003ctd\u003e0.001278\u003c/td\u003e\n","      \u003ctd\u003e0.280320\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003eFashionMNISTModelV2\u003c/td\u003e\n","      \u003ctd\u003e0.000676\u003c/td\u003e\n","      \u003ctd\u003e0.320257\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d699262f-d3b1-4ff9-bcae-9271815916ac')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-d699262f-d3b1-4ff9-bcae-9271815916ac button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d699262f-d3b1-4ff9-bcae-9271815916ac');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["            model_name  model_loss  model_acc\n","0  FashionMNISTModelV0    0.001061   0.300289\n","1  FashionMNISTModelV1    0.001278   0.280320\n","2  FashionMNISTModelV2    0.000676   0.320257"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["# compare results\n","\n","import pandas as pd\n","compare_results = pd.DataFrame([model_0_results,model_1_results,model_2_results])\n","compare_results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fXjVMgPQXJI1"},"outputs":[{"data":{"text/plain":["Text(0, 0.5, 'model')"]},"execution_count":46,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 640x480 with 1 Axes\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["# Visualize our model results\n","compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind = \"barh\")\n","plt.xlabel(\"accuracy (%)\")\n","plt.ylabel(\"model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uSI8-UP_2v9j"},"outputs":[],"source":["def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n","  pred_probs = []\n","  model.eval()\n","  with torch.inference_mode():\n","    for sample in data:\n","      sample = torch.unsqueeze(sample, dim = 0).to(device)\n","\n","      # forward \n","      pred_logit = model(sample)\n","\n","      # Get prediction probaility\n","      pred_prob = torch.softmax(pred_logit.squeeze(), dim = 0)\n","\n","      # Get pred_prob off GPU for further calculations\n","      pred_probs.append(pred_prob.cpu())\n","\n","  return torch.stack(pred_probs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"b44xNRJa3jJj"},"outputs":[],"source":["import random\n","random.seed(42)\n","test_samples = []\n","test_labels = []\n","\n","for sample, label in random.sample(list(test_data), k = 9):\n","  test_samples.append(sample)\n","  test_labels.append(label)\n","\n","\n","\n","print(f\"Test sample image shape: {test_samples[0].shape}\\Test sample label: {test_labels[0]} ({class_names[test_labels[0]]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZSifnm5j4g3O"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP+qV5ccGVTQrggEw7oCgoJ","name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}