# -*- coding: utf-8 -*-
"""Pytorch_Deep_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nETH9QF20xwR7Yy1aBmcAu3LrieWdp2z

# Yeni Bölüm
"""

!nvidia-smi

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
print(torch.__version__)

"""## Introduction to Tensors

###  Creating Tensors

Pytorch tensors are created using torch.Tensor()


"""

#scalar
scalar = torch.tensor(7)
scalar

scalar.item()

# Vector
vector = torch.tensor([7,7])
vector

vector.ndim

vector.shape

# MATRIX
MATRIX = torch.tensor([[7,8],
                       [9,10]])
MATRIX

# TENSOR
TENSOR = torch.tensor([[1,2,3],
                       [4,5,6],
                       [7,8,9]])
TENSOR

"""### Random tensors
Why random tensors?
Random tensors are important because thw ay many neurall networks learn is that they start with tensors full of random numbers and then adjust those random numbers to better represent the data.



```
Start with random numbers -> look at data-> uptade random numbers-> look at data-> uptade random numbers
```
Torch random sensors : https://pytorch.org/docs/stable/generated/torch.rand.html
"""

# Create a rndom  tensors of size( 3,4)
random_tensor = torch.rand(3,3,4)
random_tensor

#create a  random tensor with simialar shape to an image tensor
random_image_size_tensor = torch.rand(size = (3,224
                                              
                                              ,224)) # height, weight, column, R, G, B
random_image_size_tensor.shape, random_image_size_tensor.ndim

"""## Zeros and ones"""

zeros = torch.zeros( size= (3,4))
zeros

zeros * random_tensor

ones = torch.ones (size = (3,4))
ones

print(ones.dtype)
print(random_tensor.dtype)
print(zeros.dtype)

"""## Creating a range of tensors and tensors-like"""

## Use torch.range()
print(torch.range(0,10))
print(torch.arange(0,10))

one_to_ten = torch.arange(1,11,1)
one_to_ten

##Creating tensors like 
ten_zeros = torch.zeros_like(input = one_to_ten )
ten_zeros

"""##Tensor Datatypes
 **Note:** Tensor datatypes is one of the 3 big errors you'll run into Pytorch & deep learning:
1. Tensors not right datatype
2. Tensors not right shape
3. Tensors not on right device
"""

# Float 32 tensor
float_32_tensor = torch.tensor([3.0,6.0,9.0],
                               dtype = None,
                               device =None ,#cpu,#cuda What devis is your tensor on?
                               requires_grad = False, # whether or not to track gradients with this tensor operations)
)
float_32_tensor

float_32_tensor.dtype

float_16_tensor = float_32_tensor.type(torch.float16)
 float_16_tensor

float_16_tensor * float_32_tensor

int_32_tensor = torch.tensor([3,6,9], dtype= torch.int32)
int_32_tensor

float_16_tensor * int_32_tensor

"""

###Getting İnformation from Tensors(tensor attribute)

1. Tensors not right datatype --- to do get datatype from  a tensor, can Use tensor.dtype
2. Tensors not right shape --- to get shape from a  tensor, can use tensor.shape
3. Tensors not on right device --- tp get device from a tensor, can use tensor.device"""

## Create a tensor 
some_tensor = torch.rand(3,4)
some_tensor

some_tensor.size, some_tensor.shape

# find out details about some tensor 
print(some_tensor)
print(f'Datatype of tensor: {some_tensor.dtype}')
print(f'Shape of tensor: {some_tensor.shape}')
print(f'Device tensor is on : {some_tensor.device}')



"""### Tensor Operations(Monipulating Tensors)

Tensor operations include:

*Addition

*Substraction

*Multiplication

*Division

*Matrix Mult.
"""

tensor = torch.tensor([1,2,3])
tensor + 10

tensor * 10

torch.mul(tensor,10)



## Element Vise  multip.
print (tensor ,"*",tensor)
print(f'Equeals: {tensor* tensor}')

## Matrix Multip.
torch.matmul(tensor, tensor)

# Commented out IPython magic to ensure Python compatibility.
#  %%time
 value = 0
 for i in range(len(tensor)):
   value = tensor[i] * tensor[i]
print(value)

# Commented out IPython magic to ensure Python compatibility.
# %%time 
# torch.matmul(tensor,tensor)

##





"""### Find the min,max,mean,sum,etc(tensor aggregation)

"""

x = torch.arange(0,100,10)
x

#find the min 
torch.min(x),x.min()

##Find the max
torch.max(x), x.max()

## find the mean
torch.mean(x.type(torch.float32)), x.type(torch.float32).mean()

"""## Reshaping, stacking, squeezing and unsqueezing tensors

"""

import torch
x = torch.arange(1.,10.)
x, x.shape

## Add an extra dimension
x_reshaped = x.reshape(1,9) 
x_reshaped,x_reshaped.shape

## Change the view

z = x.view(1,9)
z,z.shape

# Changing z changes x(because a view of a tensor shares the same memory as the original memory)
z[:,0] = 5
z,x

# stack tensors on top of each other
# stack:Concatenates a sequence of tensors along a new dimension.
x_stacked = torch.stack([x,x,x,x],dim = 0)
x_stacked,x_stacked.shape

# torch.sequeeze() - removes all single dimensions from a target tensor
x_reshaped,x_reshaped.shape

print(f"Previous tensor: {x_reshaped}")
print(f"Previous shape: {x_reshaped.shape}")

# remove extra dimensions from x_reshaped
x_squeezed = x_reshaped.squeeze()
print(f"\nNew tensor: {x_squeezed}")
print(f"Previous shape: {x_squeezed.shape}")

x_reshaped.squeeze()

x_reshaped.squeeze().shape

# torch.unsqueeze() - adds a single dimension to a target tensor at a specific dim
print(f"Previous tensor: {x_squeezed}")
print(f"Previous shape: {x_squeezed.shape}")

x_unsqueezed = x_squeezed.unsqueeze(dim =0)
print(f"\nNew tensor: {x_unsqueezed}")
print(f"Previous shape: {x_unsqueezed.shape}")

# torch.permut -Returns a view of the original tensor input with its dimension permuted(revenge)
x_original = torch.rand(size = (224,224,3))# image tensor

# permute the original tensor to rearrange the axis(or dim) order
x_permuted = x_original.permute(2,0,1) # shift axis 0 ->1 ,1->2,2->0

print(f'Previous shape: {x_original.shape}')
print(f'New shape: {x_permuted.shape}')

x_original[0,0,0]= 725123
x_original[0,0,0], x_permuted[0,0,0]

"""### İndexing(selectin data from tensors)
Indexing with Pytorch is  similar to Indexing with Numpy
"""

# Create a tensor
import torch 
x = torch.arange(1,10).reshape(1,3,3)
x, x.shape

x[0]

# lets index on the middle bracket(dim = 1)
x[0,0], x[0][0]

# lets index on the most inner bracket(last dimension)
# ilk hücre matrisi, 2.sücre hangi satır,3.hücre hangi sütun
x[0][1][0]

### You can use ":" to select 'all' of a target dimension
 x[:,0]

# index on x to return 9
print(x[0][2][2])

# sağdaki satır
print(x[0,:,2])

"""## Pytorch tensors & Numpy
Numpy is a popular scientific  python numerical computing library
And becauseof the this , Pytorch  has  functionality to interact with it

* Data in numpy, want in Pytorch tensor ---> 'torch.from_numpy(ndarray)'
* PyTorch Tensor -> Numpy -> torch.Tensor.numpy()
"""

import torch
import numpy as np
array = np.arange(1.0,8.0)
tensor  = torch.from_numpy(array) # warning: when converting from numpy -> pytorch, pytorch reflects numpy's default datatype of float64 unless specified otherwise
array, tensor

tensor.dtype

# Change the value of array ,what will this do to 'tensor'? tensor değişmedi
array  = array +1
array ,tensor

# Tensor to Numpy array 
tensor = torch.ones(7)
numpy_tensor = tensor.numpy()
tensor, numpy_tensor

#Change the tensor, what happens to 'numpy_tensor'?
tensor = tensor +1
tensor, numpy_tensor

"""### Reproducbility (trying to tak random out of random)
In short how a neural network learns:
start with random numbers -> tensor operations -> update random numbers to try and make tehm better representations of the data -> again-> again-> again


"""



import torch
#Create two random tensors
random_tensor_A = torch.rand(3,4)
random_tensor_B = torch.rand(3,4)
print(random_tensor_A)
print(random_tensor_B)
print(random_tensor_A == random_tensor_B)

import numpy as np
x = np.random.rand(4, 5)

y = np.sum(x, axis=1)
y.shape

import torch
import random

# # Set the random seed
RANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below
torch.manual_seed(seed=RANDOM_SEED) 
random_tensor_C = torch.rand(3, 4)

# Have to reset the seed every time a new rand() is called 
# Without this, tensor_D would be different to tensor_C 
torch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens
random_tensor_D = torch.rand(3, 4)

print(f"Tensor C:\n{random_tensor_C}\n")
print(f"Tensor D:\n{random_tensor_D}\n")
print(f"Does Tensor C equal Tensor D? (anywhere)")
random_tensor_C == random_tensor_D



